<!DOCTYPE html>
<html>
<head>
    <style>
        body {
            margin: 0;
            padding: 20px;
            font-family: system-ui;
            background: #f0f0f0;
        }
        .container {
            display: grid;
            grid-template-columns: 2fr 1fr;
            gap: 20px;
            height: 100vh;
        }
        .visualization {
            background: white;
            border-radius: 8px;
            padding: 20px;
            box-shadow: 0 2px 4px rgba(0,0,0,0.1);
        }
        .explanation {
            background: white;
            border-radius: 8px;
            padding: 20px;
            box-shadow: 0 2px 4px rgba(0,0,0,0.1);
            overflow-y: auto;
        }
        .component {
            margin-bottom: 20px;
        }
        .step {
            margin-bottom: 15px;
            padding: 10px;
            border-left: 4px solid #0066cc;
            background: #f8f9fa;
        }
        svg {
            max-width: 100%;
            height: auto;
        }
        .highlight {
            color: #0066cc;
            font-weight: bold;
        }
    </style>
</head>
<body>
    <div class="container">
        <div class="visualization">
            <h2>AI Maze Explorer - System Architecture</h2>
            <svg viewBox="0 0 800 600">
                <!-- Neural Network Architecture -->
                <g transform="translate(50,50)">
                    <!-- Input Layer -->
                    <rect x="0" y="0" width="150" height="200" fill="#e3f2fd" stroke="#1565c0"/>
                    <text x="75" y="100" text-anchor="middle">Input Layer
                        <tspan x="75" y="120">(4 channels)</tspan>
                        <tspan x="75" y="140">- Walls</tspan>
                        <tspan x="75" y="160">- Visited</tspan>
                        <tspan x="75" y="180">- Position</tspan>
                        <tspan x="75" y="200">- Goal</tspan>
                    </text>

                    <!-- Convolutional Layers -->
                    <g transform="translate(200,0)">
                        <rect width="150" height="200" fill="#e8f5e9" stroke="#2e7d32"/>
                        <text x="75" y="100" text-anchor="middle">Conv Layers
                            <tspan x="75" y="120">32 filters</tspan>
                            <tspan x="75" y="140">64 filters</tspan>
                            <tspan x="75" y="160">64 filters</tspan>
                        </text>
                    </g>

                    <!-- Fully Connected Layers -->
                    <g transform="translate(400,0)">
                        <rect width="150" height="200" fill="#fff3e0" stroke="#ef6c00"/>
                        <text x="75" y="100" text-anchor="middle">FC Layers
                            <tspan x="75" y="120">512 neurons</tspan>
                            <tspan x="75" y="140">4 outputs</tspan>
                        </text>
                    </g>

                    <!-- Action Output -->
                    <g transform="translate(600,0)">
                        <rect width="150" height="200" fill="#f3e5f5" stroke="#7b1fa2"/>
                        <text x="75" y="100" text-anchor="middle">Actions
                            <tspan x="75" y="120">Up</tspan>
                            <tspan x="75" y="140">Down</tspan>
                            <tspan x="75" y="160">Left</tspan>
                            <tspan x="75" y="180">Right</tspan>
                        </text>
                    </g>

                    <!-- Connecting arrows -->
                    <path d="M150,100 H200" stroke="#666" stroke-width="2" marker-end="url(#arrowhead)"/>
                    <path d="M350,100 H400" stroke="#666" stroke-width="2" marker-end="url(#arrowhead)"/>
                    <path d="M550,100 H600" stroke="#666" stroke-width="2" marker-end="url(#arrowhead)"/>

                    <!-- Arrow marker definition -->
                    <defs>
                        <marker id="arrowhead" markerWidth="10" markerHeight="7" refX="9" refY="3.5" orient="auto">
                            <polygon points="0 0, 10 3.5, 0 7" fill="#666"/>
                        </marker>
                    </defs>
                </g>

                <!-- Reinforcement Learning Process -->
                <g transform="translate(50,300)">
                    <rect width="700" height="250" fill="#fafafa" stroke="#999"/>
                    <text x="350" y="30" text-anchor="middle" font-size="16" font-weight="bold">Reinforcement Learning Process</text>
                    
                    <!-- Process steps -->
                    <g transform="translate(50,50)">
                        <rect width="120" height="60" fill="#e3f2fd" stroke="#1565c0"/>
                        <text x="60" y="35" text-anchor="middle">State</text>
                    </g>

                    <g transform="translate(220,50)">
                        <rect width="120" height="60" fill="#e8f5e9" stroke="#2e7d32"/>
                        <text x="60" y="35" text-anchor="middle">Action</text>
                    </g>

                    <g transform="translate(390,50)">
                        <rect width="120" height="60" fill="#fff3e0" stroke="#ef6c00"/>
                        <text x="60" y="35" text-anchor="middle">Reward</text>
                    </g>

                    <g transform="translate(560,50)">
                        <rect width="120" height="60" fill="#f3e5f5" stroke="#7b1fa2"/>
                        <text x="60" y="35" text-anchor="middle">Next State</text>
                    </g>

                    <!-- Connecting arrows -->
                    <path d="M170,80 H220" stroke="#666" stroke-width="2" marker-end="url(#arrowhead)"/>
                    <path d="M340,80 H390" stroke="#666" stroke-width="2" marker-end="url(#arrowhead)"/>
                    <path d="M510,80 H560" stroke="#666" stroke-width="2" marker-end="url(#arrowhead)"/>
                    <path d="M620,110 C620,160 120,160 120,110" stroke="#666" stroke-width="2" marker-end="url(#arrowhead)" fill="none"/>
                </g>
            </svg>
        </div>

        <div class="explanation">
            <h2>How the AI Maze Explorer Works</h2>
            
            <div class="component">
                <h3>1. Neural Network Architecture</h3>
                <div class="step">
                    <p>The AI uses a <span class="highlight">Deep Q-Network (DQN)</span> with:</p>
                    <ul>
                        <li>Input: 4-channel state representation (walls, visited areas, current position, goal)</li>
                        <li>3 Convolutional layers for processing spatial information</li>
                        <li>2 Fully connected layers for decision making</li>
                        <li>Output: 4 possible actions (up, down, left, right)</li>
                    </ul>
                </div>
            </div>

            <div class="component">
                <h3>2. Learning Process</h3>
                <div class="step">
                    <p>The AI learns through <span class="highlight">reinforcement learning</span>:</p>
                    <ul>
                        <li>Explores the maze using an epsilon-greedy strategy</li>
                        <li>Receives rewards for:
                            <ul>
                                <li>Discovering new areas (+0.5)</li>
                                <li>Reaching the goal (+100)</li>
                                <li>Each step (-0.1)</li>
                                <li>Hitting walls (-1)</li>
                            </ul>
                        </li>
                        <li>Updates its policy using experience replay</li>
                    </ul>
                </div>
            </div>

            <div class="component">
                <h3>3. Vision System</h3>
                <div class="step">
                    <p>The AI has a limited vision range of 5 cells in each direction, creating a:</p>
                    <ul>
                        <li>11x11 observation window (VISION_RANGE * 2 + 1)</li>
                        <li>Local view of walls and visited areas</li>
                        <li>Awareness of goal position within vision range</li>
                    </ul>
                </div>
            </div>

            <div class="component">
                <h3>4. Exploration Strategy</h3>
                <div class="step">
                    <p>Uses an <span class="highlight">epsilon-greedy</span> approach:</p>
                    <ul>
                        <li>Starts with 100% random actions (epsilon = 1.0)</li>
                        <li>Gradually reduces randomness to 1% (epsilon = 0.01)</li>
                        <li>Balances exploration and exploitation</li>
                    </ul>
                </div>
            </div>

            <div class="component">
                <h3>5. Performance Metrics</h3>
                <div class="step">
                    <p>Tracks learning progress through:</p>
                    <ul>
                        <li>Success rate in reaching goals</li>
                        <li>Average reward per episode</li>
                        <li>Total episodes completed</li>
                    </ul>
                </div>
            </div>
        </div>
    </div>
</body>
</html>